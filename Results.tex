\documentclass{unswmaths}
\usepackage[a4paper]{geometry}
\usepackage{fancyhdr}
\pagestyle{fancy}
\usepackage{cite}
\begin{document}

\setlength\parindent{0pt}
\setlength{\parskip}{5mm plus4mm minus3mm}

\unswtitle{Adam J. Gray}{3329798}{Thesis Introduction}{Fractional Differential Equations}
\fancyfoot[l]{Adam J. Gray}
\fancyfoot[r]{\today}
\fancyhead[l]{The University of New South Wales}
\fancyhead[r]{Fractional Differential Equations}

\section*{Useful Results in ODE Theory}

\begin{unswlem}[Gronwell-Bellman Lemma]
\label{lem:Gronwell_Bellman}
Let $ \rho : [a,b] \longrightarrow [0, \infty) $ be a piecewise continuous function
and let $ \tau \in [a,b] $. If there exists a set of non-negative constants
$ \{ K_0, K_1, K_2 \} $ such that for all $ t \in [a,b] $
$$ \rho(t) \leq K_0 + K_1|t-\tau| + |\int_\tau^t K_2 \rho(s) ds | $$
then for all $ t \in [a,b] $ we have that
\begin{align*}
	\rho(t) \leq 
	\begin{cases}
		K_0\exp(K_2|t-\tau|) + \frac{K_1}{K_2}\left[ \exp(K_2|t-\tau|) - 1 \right] & \text{ if } K_2 > 0 \\
		K_0 + K_1|t-\tau| & \text{ if } K_2 = 0 
	\end{cases}
\end{align*}
\end{unswlem}
\begin{proof}
We prove this result in two parts, first for $ t \in [ \tau, b ] $ and then
for $ t \in [ a, \tau ] $. 

Let $ t \in [\tau, b] $. In the case where $ K_2 = 0 $ then it is trivial to see 
that $ \rho(t) \leq K_0 + K_1|t - \tau| $ so assume that $ K_2 > 0 $. 

Set $$ h(t) = K_0 + K_1|t - \tau| + | \int_\tau^t K_2 \rho(s) ds | $$ then note
that $ \rho(t) \leq h(t) $ and $ h(\tau) = K_0 $. 

Now see that
\begin{align*} 
	h'(t) &= K_1 + K_2\rho(t)  \\
	h'(t) &\leq K_1 + K_2h(t).
\end{align*}
We therefore solve the differential inequality
\begin{align*}
	h'(t) - K_2h(t) \leq K_1. 
\end{align*}

By multiplying by the integrating factor $ \exp(-K_2 t) $ and applying the 
product rule we get that 
\begin{align*}
	\frac{d}{dt} \left[ \exp(-K_2t)h(t) \right] &\leq K_1\exp(-K_2t)
\end{align*}
and integrating both sides from $ \tau \text{ to } t $ yields, 
\begin{align*}
	\exp(-K_2t)h(t) - \exp(-K_2\tau)h(\tau) \leq K_1 \int_\tau^t \exp(-K_2s)ds.
\end{align*}
By evaluating the integral we have that 
\begin{align*}
	\exp(-K_2t)h(t) - \exp(-K_2\tau)h(\tau) \leq \frac{-K_1}{K_2}\left[ \exp(-K_2t) - \exp(-K_2\tau) \right]
\end{align*}
and then rearranging to get $ h(t) $ as the subject we get
\begin{equation}
	\label{eq:Gronwell_Bellman_1}
	h(t) \leq \exp(K_2(t-\tau))h(\tau) + \frac{K_1}{K_2}\left[\exp(K_2|t-\tau|) - 1\right].
\end{equation}

Now consider the case when $ t \in [a, \tau] $.
Again when $ K_2 = 0 $, we can immediately see that $ \rho(t) \leq K_0 + K_1|t - \tau| $, 
so we assume that $ K_2 > 0 $. 
Again we let 
\begin{align*}
	h(t) 	&= K_0 + K_1|t - \tau| + |\int_\tau^t K_2 \rho(s) ds | \\
	     	&= K_0 + K_1(\tau - t) + \int_t^\tau K_2\rho(s) ds
\end{align*}
and note that $ \rho(t) \leq h(t) $. Differentiating $ h(t) $ we get
\begin{align*}
	h'(t) = -K_1 +K_2\rho(t)
\end{align*}
with $ h(\tau) = K_0 $.

By substituting $ h(t) $ for $ \rho(t) $ and rearranging we get that
$ h'(t) +K_2h(t) \geq -K_1 $.

Multiplying both sides by the integrating factor $ \exp(K_2t) $ and applying the 
product rule we get 
\begin{align*}
	\frac{d}{dt}\left[ \exp(K_2t)h(t) \right] \geq K_1\exp(K_2t)
\end{align*}
and by integrating both sides from $ t \text{ to } \tau $ we get
\begin{align*}
	\exp(\tau K_2)h(\tau) - \exp(tK_2)h(t) \geq \int_t^\tau K_1 \exp(K_2s)ds.
\end{align*}
Evaluating the integral and rearranging to get $ h(t) $ the subject we get
\begin{equation}
\label{eq:Gronwell_Bellman_2}
	h(t) \leq K_0\exp(K_2|t - \tau|) + \frac{K_1}{K_2}\left[\exp(K_2|t-\tau|) - 1\right].
\end{equation}
Now in both cases we have that $ \rho(t) \leq h(t) $ so from 
\eqref{eq:Gronwell_Bellman_1} and \eqref{eq:Gronwell_Bellman_2} the result immediately follows.
\end{proof}
We will use this corollary in other results so we put it here for completeness.
\begin{unswcor}[Corollary to the Gronwell-Bellman Lemma]
\label{cor:Gronwell_Bellman}
Let $ \rho : [a,b] \longrightarrow [0, \infty) $ be a continuous function and
let $ \tau \in [a,b] $. If there is a non-negative constant $ K_2 $ such that for
all $ t \in [a,b] $
$$
	\rho(t) \leq | \int_\tau^t K_2 \rho(s) ds |
$$
then $ \rho(t) \equiv 0 $ for all $ t \in [a, b] $.
\end{unswcor}
\begin{proof}
Applying lemma \ref{lem:Gronwell_Bellman} with $ K_0 = K_1 = 0 $ we see that
$ \rho(t) \leq 0 $ for all $ t \in [a,b] $, but since by assumption $ \rho(t) \geq 0 $ 
for all $ t \in [a, b] $ it must be that $ \rho(t) \equiv 0 $ on $ [a, b] $.	
\end{proof}

In some sense the Gronwell Bellman lemma put an a-priori bound on the solution
given an integral inequality.

Naturally if $ \alpha $ and $  \beta $ are piecewise continuous, non-negative functions on
$ [a, b] $ and we have the following inequality
\begin{align*}
	\rho(t) \leq \alpha(t) + C|t - \tau| + | \int_\tau^t \beta(s)\rho(s) ds|
\end{align*}
then we could set $ K_0 = \max_{t \in [a,b]}\{\alpha(t)\}, K_1 = C, K_2 = \max_{t \in [a,b]}\{\beta(t)\} $
and apply lemma \ref{lem:Gronwell_Bellman}, however we can improve this bound by replicating the
techniques used the proof of \ref{lem:Gronwell_Bellman}. This leads us to the following lemma.
\begin{unswlem}[Extended Gronwell Bellman Lemma]
\label{lem:Extended_Gronwell_Bellman}
Let $ \rho : [a,b] \longrightarrow [0, \infty) $ be a piecewise continuous function
and let $ \tau \in [a,b] $. If there exists a non-negative constant
$ K $ and non-negative piecewise continuous functions $ \alpha \in C^2[a,b] $ and $ \beta \in C^1[a,b] $
such that for all $ t \in [a,b] $
$$ \rho(t) \leq \alpha(t) + K|t-\tau| + |\int_\tau^t \beta(t) \rho(s) ds | $$
then for all $ t \in [a,b] $ we have that
\begin{align*}
    \rho(t) \leq \frac{\int_\tau^t \alpha'(s)\left( \exp(- \int_\tau^s \beta(r)dr) \right)ds + K|t - \tau| + \alpha(\tau) }{\exp(-\int_\tau^t\beta(s)ds)} 
\end{align*}
\end{unswlem}
\begin{proof}
TODO
\end{proof}
Now lemma \ref{lem:Gronwell_Bellman} is a sepecial case of lemma \ref{lem:Extended_Gronwell_Bellman}
although in most cases lemma 1 will suffice.

\begin{unswlem}[Non-multiplicity of Solutions]
\label{lem:Non_Multiplicity}
Let $ D \subseteq \mathbb{R} $ and let $ f : D \longrightarrow \mathbb{R} $ be continuous.
Let $ (\tau, A) \in D $ and consider the IVP
\begin{align}
	\label{eq:IVP_Diff}
	x'(t) &= f(t,x(t)) \\
	\label{eq:IVP_IV}
	x(\tau) &= A
\end{align}
If there exists a constant $ L > 0 $ such that 
$$
	|f(t,u) - f(t,v)| \leq L|u-v| 
$$
for all $ (t,u),(t,v) \in D $ then the IVP given in \eqref{eq:IVP_Diff} and \eqref{eq:IVP_IV}
has at most one solution, whose graph lies in D.
\end{unswlem}
\begin{proof}
Let  $ x = x(t) \text{ and } y = y(t) $ be two solutions to \eqref{eq:IVP_Diff} and 
\eqref{eq:IVP_IV}.
Then we have the equivalent integral representations
\begin{align*}
x(t) 	&= A + \int_\tau^t f(s,x(s))ds \\
y(t)	&= A + \int_\tau^t f(s,y(s))ds.
\end{align*}
Now consider $ \rho(t) = |x(t) - y(t)| $ and note that it would suffice to show that
$ \rho(t) \equiv 0 $. 
See that,
\begin{align*}
\rho(t) 	&= |\int_\tau^t f(s,x(s)) - f(s,y(s))ds| \\
		&\leq \int_\tau^t|f(s,x(s))-f(s,y(s))|ds \\
		&\leq \int_\tau^t L|x(s)-y(s)| ds \\
		&= \int_\tau^t L|\rho(s)|ds.
\end{align*}
Corollary \ref{cor:Gronwell_Bellman} then implies that  $ \rho(t) \equiv 0 $ 
and hence $ x(t) \equiv y(t) $, which is to say that there is at most one solution to
\eqref{eq:IVP_Diff} and \eqref{eq:IVP_IV}.
\end{proof}

For future theorems and lemmas we shall use the following definitions, unless otherwise 
stated.

Let $ D \subseteq \mathbb{R}^2 $ and let 
\begin{equation}
	\label{def:IVP_Gen_Fun}
	f : D \longrightarrow \mathbb{R}
\end{equation}
be a continuous. 

Let $ A \text{ and } \tau $ be constants and define the IVP
\begin{align}
	\label{def:IVP_Diff}
	x'(t)	&= f(t,x(t)) \\
	\label{def:IVP_IV}
	x(\tau) &= A.
\end{align}

Define the sequence of functions 
\begin{align}
	\label{def:Succ_Approx_Seq}
	\left\{ \phi_k (t) \right\}_{k=0}^{\infty}
\end{align}
by 
\begin{align*}
	\phi_0(t) &= A \\  
	\phi_k(t) &= A + \int_\tau^t f(s,\phi_{k-1}(s))ds
\end{align*}

Let $ B > 0 $ be some constant then define the rectable 
\begin{align}
	\label{def:Rectangle}
	\mathcal{R}_{[a,b]} = \{ (t,p) \in \mathbb{R}^2 : t \in [a,b], |p-A| \leq B \}. 
\end{align}

Let
\begin{align*}
	\alpha 	&= \min\{\tau - a, \frac{B}{M} \} \\
	\beta	&= \min\{b - \tau, \frac{B}{M} \}
\end{align*}
and define the interval 
\begin{align}
	\label{def:Interval}
	I = [ \tau - \alpha, \tau + \beta ]
\end{align}
\begin{unswlem}
	If $ f : \mathcal{R}_{[a,b]} \longrightarrow \mathbb{R} $ then each $ \phi_k(t) $ is is continuous.
\end{unswlem}
\begin{proof}
	Consider 
	\begin{align*}
		| \phi_k (t_1) - \phi_k (t_2) | 	&= | \int_\tau^{t_1} f(s,\phi_{k-1}(s))ds - \int_\tau^{t_2} f(s,\phi_{k-1}(s))ds | \\
							&\leq | \int_{t_2}^{t_1} | f(s, \phi_{k-1}(s)) | ds | \\
							&\leq | \int_{t_1}^{t_2} M ds | \\
							&= M|t_1 - t_2|
	\end{align*}
	where $ M = \sup\{f(u,v) | (u,v) \in \mathcal{R}_{[a,b]} \} $.
	We therefore have that for any $ \varepsilon > 0 $ there exists a $ \delta = \frac{\varepsilon}{M} $ such that 
	$ | \phi_k (t_1) - \phi_k (t_2) | \leq \varepsilon $ whenever $ | t_1 - t_2 | < \delta $.
\end{proof}

\begin{unswlem}[Existence: Picard - Lindelof]
	\label{lem:Picard_Lindelof}
	Let $ f : \mathcal{R}_{[a,b]} \longrightarrow \mathbb{R} $ be a continuous function. 
	If there exists a constant $ L > 0 $ satisfying $ |f(t,u) - f(t,v) \leq L|u-v| $ for all
	$ (t, u), (t,v) \in \mathcal{R}_{[a,b]} $ then the IVP defined by \eqref{def:IVP_Diff} and \eqref{def:IVP_IV} 
	has a unique solution on $ I $, with $ (t, x(t) ) \in \mathcal{R}_{[\tau - \alpha, \tau + \beta]} $ 
	for all $ t \in I $.
\end{unswlem}
\begin{proof}
	Since $ f $ is Lipschitz there is at most on solution to \eqref{def:IVP_Diff} and \eqref{def:IVP_IV}. This is
	guarenteed by lemma \ref{lem:Non_Multiplicity}. 
	We also know, from lemma \ref{lem:Picard_Lindelof}, that $ \phi_k(t) $ are all continuous and it is not hard to 
	
	Now suppose $ \phi_k \longrightarrow \phi $ uniformly in $ I $, then we have that
	\begin{itemize}
		\item $ \phi $ is continuous on $ I $ \\
		\item $ (t, \phi(t)) \in \mathcal{R}_{[\tau-\alpha, \tau + \beta]} $
		\item 	\begin{align*}
				\lim_{k \longrightarrow \infty} \phi_k (t) 	&= \lim_{k \longrightarrow \infty} \left( A + \int_\tau^t f(s,\phi_{k-1}(s))ds \right) \\
				\phi(t)						&= A + \int_\tau^t f(s, \phi(s)) ds
			\end{align*}
	\end{itemize}
	So we just have to show that $ \phi_k \longrightarrow \phi $ uniformly on $ I $.
	Now note that 
	$ \phi_k(t) = \phi_0(t) + \sum_{i=1}^k (\phi_i(t) - \phi_{i-1}(t)) $
	so to show that $ \phi_k $ converges uniformly we apply the Weierstrass M Test. 
	So we wish to consider $ \phi(t) - \phi_{i-1}(t)| $ and we claim that 
	\begin{align}
		\label{eq:Picard_Lindelof_Ineq}
		| \phi(t) - \phi_{i-1}(t)| \leq \frac{ML^{i-1} |t - \tau|^i}{i!}
	\end{align}
	
	We prove this by induction.
	In the case $ i = 0 $,
	\begin{align*}
		|\phi_1 (t) - \phi_0 (t)| 
		&= | A + \int_\tau^t f(s, \phi_0(s))ds - A | \\
		&= | \int_\tau^t f(s, A) ds | \\
		&\leq |\int_\tau^t |f(s,A)|ds | \\
		\&leq M|t - \tau|.
	\end{align*}
	So the claim holds in the base case.
	Assuming that \eqref{eq:Picard_Lindelof_Ineq} holds for some $ i = n \geq 1$ then
	\begin{align*}
		|\phi_{n+1}(t) - \phi_{n}(t) | &= | \left(A +\int_\tau^t f(s,\phi_{n+1}(s)) ds\right) - \left(A + \int_\tau^t f(s, \phi_{n-1}(s)) ds \right) | \\
		&= | \int_\tau^t f(s, \phi_n(s)) - f(s, \phi_{n-1}(s)) ds | \\
		&\leq | \int_\tau^t f(s, \phi_n(s))) - f(s, \phi_{n-1}(s)) | ds \\
		&\leq L | \int_\tau^t |\phi_n(s) - \phi_{n-1}(s)| ds | \\
		&\leq | \int^t_\tau \frac{L^nM(s-\tau)^n}{n!} ds| \\
		&\leq \frac{ML^n(t-\tau)^{n+1}}{(n+1)!}.
	\end{align*}
	So we have that \eqref{eq:Picard_Lindelof_Ineq} holds for $ i = n + 1 $ and so it holds for all $ i \geq 0 $.
	Now it is clear to see that
	\begin{align*}
		\sum_{i=0}^\infty \frac{ML^i-1(t - \tau)^{i}}{(i)!} < \infty
	\end{align*}
	so by the Wierstrass M Test $ \phi_{k} \longrightarrow \phi $ uniformly on $ [\tau, \tau + \beta] $,
	with $ \phi $ a solution to \eqref{def:IVP_Diff} and \eqref{def:IVP_IV} on $ [\tau, \tau + \beta] $.
	TODO: Extend.
\end{proof}

We now wish to introduce some other definitions and theorems which are useful when considering the
existence the uniqueness of ODEs.
\begin{unswdef}
	\label{def:Ctr_Map}
	Let $ (M_1,d_1) $, $ (M_2, d_2) $ be some metric spaces then a map $ T : M_1 \longrightarrow M_2 $ is
	a contraction mapping if there exists some real number $ \gamma $ such that for all $ x, y \in M_1 $
	$$ d_2(T(x), T(y)) \leq \gamma d_1(x,y) $$.
\end{unswdef}

\begin{unswlem}[Continuity of Contraction Mappings]
	\label{lem:Cont_Ctr_Map}
	If $ (M_1,d_1) $, $ (M_2, d_2) $ are some metric spaces then a contraction mapping 
	$ T:M_1 \longrightarrow M_2 $ is uniformly continuous. 
\end{unswlem}
\begin{proof}
	Given any $ \varepsilon > 0 $ there exists a $ \delta = \varepsilon $ such that whenever
	$ d_1 (x, y) \leq \delta $, we have that $ d_2(x,y) \leq \varepsilon $. So $ T $ must be uniformly
	continuous.
\end{proof}

\begin{unswthm}[Banach Fixed Point Theorem]
	Let $ (M, d) $ be a non-empty complete metric space with a contraction mapping 
	$ T : M \longrightarrow M $. Then there exists a unique point $ x^* \in M $ such that
	$ T(x) = x $. Furthermore the the sequence 
	\begin{align}
		\label{eq:Banach_FPT_Seq}
		\left\{ x_n \right\}_n^\infty 
	\end{align} defined by 
	\begin{align}
		x_0 \in M \\
		x_k = T(x_{k-1}) 
	\end{align}
	is such that $ x_n \longrightarrow x^* $.
\end{unswthm}
\begin{proof}
	We first show that $ d(x_{n+1}, x_n) \leq \gamma^n d(x_1, x_0) $.
	We prove this by indution so consider the base case 
	\begin{align*}
		d(x_2, x_1) 	&= d(T(x_1), T(x_0)) \\
		d(x_2, x_1) 	&\leq \gamma d(x_1, x_0).
	\end{align*}
	Now assume that this is true for $ n $ and consider the case $ n+1 $ and see that
	\begin{align*}
		d(x_{n+2}, x_{n+1})	&= d(T(x_{n+1}, T(x_n)) \\
					&\leq \gamma d(x_{n+1}, x_n) \\
					&\leq \gamma \gamma^n d(x_1, x_0) \\
					&\leq \gamma^{n+1} d(x_1, x_0).
	\end{align*}
	So it is true for $ n + 1 $ and so we have the result by indution.
	
	Next we wish to show that the sequence $ \left\{ x_n \right\}_{n=0}^\infty $ is a Cauchy sequence.
	
	Consider $ x_n, x_m $ with $ m > n $ and note that
	\begin{align}
		d(x_m, x_n) 	&\leq d(x_m, x_{m-1}) + d(x_{m-1}, x_{m-2}) + \ldots + d(x_{n+1}, x_{n}) \nonumber \\
				&\leq \gamma^{m-1} d(x_1, x_0) + \gamma^{m-2} d(x_1, x_0) + \ldots + \gamma{n} d(x_0, x_1) \nonumber \\
				&= \gamma^n d(x_1, x_0) \sum_{k=0}^{m-n-1}\gamma^{k} \nonumber \\
				&\leq \gamma^{n}d(x_1, x_0) \sum_{k=0}^{\infty} \gamma^{k} \nonumber \\
				&= \gamma^n d(x_1, x_0) \left( \frac{1}{1-\gamma} \right).
	\end{align}
	Now for any $ \varepsilon > 0 $ there exists $ N \in \mathbb{N} $, say,
	$$ N = \left\lceil \frac{\log \left(\frac{\varepsilon(1 - \gamma)}{d(x_1, x_0)}\right)}{\log(\gamma)} \right\rceil $$ 
	such that $ n > N $ implies 
	$$ \gamma^n d(x_1, x_0) \left( \frac{1}{1-\gamma} \right) < \varepsilon. $$
	
	Since $ \varepsilon > 0 $ was arbitrary we have the fact that the sequence is Cauchy. The fact that the metric
	space $ M $ is complete guarentees that the limit of the Cauchy sequence exists.
	
	We now wish to prove that $ \lim_{n \longrightarrow \infty} x_n = x^* $ is a fixed point of $ T $.
	To do this consider
	\begin{align*}
		\lim_{n\longrightarrow \infty} x_n = \ \lim_{n \longrightarrow \infty} T(x_{n-1}) \\
	\end{align*}
	but since $ T $ is a contraction mapping, lemma \ref{lem:Cont_Ctr_Map} guarentees that $ T $ is 
	uniformly continuous, and hence 
	$$ \lim_{n \longrightarrow \infty } T(x_{n-1}) = T\left(\lim_{n \longrightarrow \infty} x_{n-1} \right) $$
	which is to say that $ x^* = T(x^*) $.
	
	It now only remains to prove the uniqueness claim.
	Suppose $ y^* $ is another fixed point of $ T $ then we must have that 
	$ d(x^*, y^*) = d(T(x^*),T(y^*)) $ and hence 
	$$ 0 \leq d(x^*, y^*) \leq \gamma d(x^*, y^*), $$
	but $ \gamma \in [0, 1) $ which implies $ 0 \leq d(x^*, y^*) \leq 0 $
	and hence $ x^* = y^* $.
	So we have show that there exists a unique point $ x^* \in M $ such that
	$ T(x) = x $ and that this point is the limit of sequence \eqref{eq:Banach_FPT_Seq}.
\end{proof}

\begin{unswlem}[Alternate Picard - Lindelof]
	Given a constant $K $ and compact $ D subseteq \mathbb{R} $ and the continuous function $ f : D \mathbb{R} $ which
	satisfies a Lipschitz condition of the form 
	$$ | f(t,u) - f(t,v) | \leq K | u - v |, $$ the IVP defined by \ref{def:IVP_Diff} and \ref{def:IVP_IV} 
	has a unique solution.
\end{unswlem}
\begin{proof}
	The IVP \ref{def:IVP_Diff}, \ref{def:IVP_IV} is equivelent to the integral equation
	\begin{align}
		\label{eq:IVP_Integral}
		x(t) - x(\tau) = \int_\tau^t f(s,x(s)) ds 
	\end{align}
	Now as $ f $ is continuous and $ D $ is compact there must exist a constant $ M $ such that
	$ | f(t,u) | \leq M $ and a neighbourhood $ D' \subseteq D $ such that $ (\tau, A) \in D' $.
	
	We can choose $ \delta $ such that 
	\begin{itemize}
		\item $ (t, u) \in D' $ if $ | t - \tau | \leq $ and $ | x(t) - A | \leq M \delta $
		\item $ K\delta < 1 $.
	\end{itemize}
	
	FINISH PROOF
	
\end{proof}
We now present a general statement of Dini's theorem.
\begin{unswthm}[Dini's Theorem]
    \label{thm:Dini}
	Let $ X $ be a compact topological space, and let $ \{ f_n \} $ be a 
	sequence of continuous real valued functions such that
	$ f_n(x) \leq f_{n+1}(x) $ for all $ n $ and $ x \in X $ and such that $ f_n \underset{\text{pointwise}}{\longrightarrow} f $
	where $ f $ is continuous. In this case $ f_n \underset{\text{uniformly}}{\longrightarrow} f $.
\end{unswthm}
\begin{proof}
    Let $ \varepsilon > 0 $. Let $ g_n = f - f_n $ and $ E_n = \{ x \in X : g_n(x) < \varepsilon \} $. Now as
    each $ g_n $ is continuous then each $ E_n $ must be open. Now as $ f_n \underset{\text{pointwise}}{\longrightarrow} f $
    $ E_n $ must form an open over of $ X $ , but by compactness there must exist an integer $ N $ such that
    $ E_N = X $ which is to say that if $ n \geq N $ then $ |f(x) - f_n(x)| \leq \varepsilon $.
    So $ f_n \underset{\text{uniformly}}{\longrightarrow} f $.
\end{proof}

\begin{unswlem}[Bihari's Inequality]
    Let $ f : [0, \infty) \longrightarrow [0, \infty) $ and $ u : [0, infty) \longrightarrow [0,\infty) $ 
    be continuous and let $ w : [0, \infty) \longrightarrow [0, \infty) $ be a non-decreasing continuous function
    such that $ w(u) > 0 $ for $ u \in (0, \infty) $.
    Let $ \alpha $ be a non-negative constant.
    If $ u $ satisfies the following inequality
    \begin{align*}
        u(t) \leq \alpha + \int_0^\infty f(s)w(u(s))ds  && t \in [0, \infty)
    \end{align*}
    then
    \begin{align*}
        u(t) \leq G^{-1}\left( G(\alpha) + \int^t_0 f(s)ds \right) && t \in [0, T]
    \end{align*}
    where
    \begin{align*}
        G(x)
    \end{align*}
\end{unswlem}
\begin{proof}
\end{proof}

\section*{Useful Results in FDE Theory}
Since the gamma function will come up often when dealing with fractional differential equations we
provide a definition of the gamma function, 
\begin{align*}
	\Gamma(z) = \int_0^\infty e^{-t}t^{z-1} dt.
\end{align*}

We also present without proof (due to triviality) some important results relating to the gamma function.

\begin{align*}
	\Gamma(n+1) 	&= n! \\
	\Gamma(z)	&= (z-1)\Gamma(z-1)
\end{align*}

A less trivial result we will prove is that
\begin{unswprp}
	$$ \Gamma(z) = \lim_{n \longrightarrow \infty} \frac{n!n^z}{\prod_{k=0}^n (z+k)}. $$
\end{unswprp}
\begin{proof}
	Consider the sequence of functions 
	\begin{align*}
		f_n(z)  	&= \int_0^n \left( 1 - \frac{t}{n} \right)^n t^{z-1} dt
	\end{align*}
	and observe that if we permit the limit to pass through the integral 
	\footnote{It is possible to prove that this is justified and the interested reader is encouraged to
	consult FIXME 
} then
	\begin{align*}
		\lim_{n \longrightarrow \infty}f_n(z) &= \Gamma(z).
	\end{align*}
	Also if we perform the substitution $ \tau = \frac{t}{n} $ and perform repeated integration by parts
	we obtain 
	\begin{align*}
		f_n(z) 	&= \int_0^1(1 - \tau)^n\tau^z-1 d\tau \\
			&= \frac{n^z}{z} n \int_0^1 (1-\tau)^{n-1}\tau ^z d\tau \\
			&= \frac{n^z n!}{\prod_{k=0}^{n - 1} (z+k)} \int_0^1\tau^{z+n-1}d\tau \\
			&= \frac{n^z}{\prod_{k=0}^{n}(z+k)}
	\end{align*}
	
\end{proof}

We would also like to introduce the beta function, as it will often be more convenient to use the beta function than
several combinations of gamma functions.

$$ B(x,y) = \int_0^1 t^{x-1}(1-t)^{y-1} dt $$

\begin{unswprp}[Relationship Between the Beta and Gamma Functions]
	$$ B(x,y) = \frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)} $$
\end{unswprp}
\begin{proof}
	Consider the integral

	\begin{align}
		\label{eq:Beta_Conv}
		h_{x,y}(t) = \int_0^t = \tau^{x-1}(1-\tau)^y-1 d\tau.
	\end{align}
	Clearly \eqref{eq:Beta_Conv} is the convolution of $ t^{x-1} $ and $ t^{y-1} $ and therefore
	$$ \mathcal{L}\left\{h_{x,y}(t)\right\} = \mathcal{L}\left\{t^{x-1}\right\} \mathcal{L}\left\{t^{y-1}\right\} $$
	but clearly we have $ \mathcal{L}(t^{x-1}) = \frac{\Gamma(x)}{s^x} $ and likewise for $ t^{y-1} $ so we have
	\begin{align*}
		\mathcal{L}\left\{ h_{x,y}(t) \right\} = \frac{\Gamma(z)\Gamma(y)}{s^{x+y}}
	\end{align*}
	The applying the inverse Laplace transform we arrive at
	$$ h_{x,y}(t) = \frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)}t^{x+y-1} $$
	but $ h_{x,y}(1) = B(x,y) $ so we have that
	$$
		B(x,y) = \frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)}
	$$
\end{proof}

PUT IN COMPLEX EVALUATION OF THE GAMMA FUNCTION

We now wish to define the parameter Mittag-Leffler functions

\begin{unswdef}
	\begin{align}	
		\label{def:Mittag_Leffler_1}
		M_\alpha (z) 		&= \sum_{k=0}^\infty \frac{x^k}{\Gamma(\alpha k + 1)} \\
		\label{def:Mittag_Leffler_2}
		M_{\alpha, \beta}(z) 	&= \sum_{k=0}^\infty \frac{x^k}{\Gamma(\alpha k + \beta)}
	\end{align}
\end{unswdef}

The one and two paramter Mittag-Leffler functions are extremly general and many elementry functions can be written in terms
of either of these functions.

For example it is clear that 
\begin{align*}
	M_{0,1}(t) 	&= \frac{1}{1-t} \\
	M_{1,1}(t)	&= \exp(t) \\
	M_{2,1}(t) 	&= \cosh(t) \\
	M_{2,2}(t)	&= \frac{\sinh(t)}{t}
\end{align*}
\end{document}
