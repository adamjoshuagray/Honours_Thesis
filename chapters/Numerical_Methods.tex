
\section{Numerical Methods for Fractional Differential Equations}

In this section we describe numerical methods which can be used for solving fractional differential equations. We being by looking at two wasy for approximating a fractional derivative and then look at a specific scheme outlined by Diethlem \cite{Diethelm2011}.

\subsection{Approximations of Fractional Derivatives and Integrals}

We first recall that the Gr\"{u}nwald-Letnikov derivative of a function $ f $ is defined by

\begin{align*}
    \glder{a}{t}{\alpha}{f}(t) = \lim_{h \lra 0} \frac{\fracdelta{a}{\alpha}{h}{f}(t)}{h^\alpha} & & \fracdelta{a}{\alpha}{h}{f}(t) = \sum_{j=0}^{\lfloor \frac{t-a}{h} \rfloor} (-1)^{j} { \alpha \choose j } f(t-jh)
\end{align*}

For a very large class of functions this coincides with the Riemann-Liouville fractional derivative \cite{Podlubny1999} which we again recall here 

\begin{align*}
    \rld{a}{t}{\alpha}{f}(t) = \left( \frac{d}{dt} \right)^{\lfloor \alpha \rfloor + 1} \int_{a}^{t}(t-\tau)^{\lfloor \alpha \rfloor - \alpha} f(\tau)d\tau.
\end{align*}

These two definitions suggest two general methods for approximating a fractional derivative.

\subsubsection{Gr\"{u}nwald-Letnikov Approximation}
In much the same was as we might use a finite difference method to approximate an integer order derivative we apply a similar approach here. By choosing some small, but non-zero value of $ h $ we can approximate the fractional derivative of a function by
\begin{align}
    \widehat{ \glder{a}{t}{\alpha}{f}(t) } = \frac{1}{h^\alpha} \sum_{j=0}^n (-1)^j {\alpha \choose j} f(t - jh).
\end{align}

It can be shown \cite{Podlubny1999} that this yeilds a first order approximation of the Riemann-Liouville fractional derivative.
\subsubsection{Quadrature Approximation}
A completely different approach to take is to use a quadrature rule to evalute the integral from the Riemann-Liouville definition. If one is simply evaluating a derivative this method is not ideal because it also requires the approximation of an integer order derivative. If, however, we wish to calculate a fractional integral, such as might be required in a numerical FDE solver, then the quadrature method may be a more practical method.

There is no \emph{set way} to do the quadrature approximation and it will all depend on the scheme being used. 

In the following sections we will examine a general initial value problem and an Adams Moulton Bashforth scheme which is based of a quadrature method. \cite{Diethelm2011}

\subsubsection{An Initial Value Problem}
Lets consider the initial value problem
\begin{align}
    \label{eq:num_ivp_1}
    \capder{0}{x}{\alpha}{y} &= f(x,y) \\
    \label{eq:num_ivp_2}
    y^{(k)}(0) &= y_{0}^{(k)} 
\end{align}
for $ 0 \leq k < \lfloor \alpha \rfloor $. Note that this initial value problem is stated in terms of Caputo derivatives. The motivation for such intial value problems was discussed in [INSERT CC REF].

One of the most important things to note about this equation is that for non-integer values of $ \alpha $ it is non-local, in the sense that the value of the solution at $ y(x_0+h) $ depends not only on $ y(x_0) $ but also on $ y(x), x \in [0, x_0] $ \cite{Diethelm2010}. This is in contrast to ordinary differential equations and this fact is what makes fractional differential equations considerably more complex to solve. Even multistep methods which can be used to solve ordinary differential equations, only rely on some \emph{fixed} number previous time steps. As a solution to a fractional differential equation progresses it relies on more and more previous time steps. As one might suspect this fundamentally increases the computational complexity of the schemes used to solve fractional differential equations as compared with the schemes used to solve traditional ordinary differential equations. \footnote{It is possible to reduce these computations to look at some large but fixed number of previous gridpoints 
but this results in a speed vs. accuracy tradeoff which will be discussed in section \ref{sec:reduce_complexity}}

\subsubsection{The Adams Moulton Bashforth Scheme}
\label{sec:amb_desc}
We briefly outline a method explained and analysed in detail in \cite{Diethelm2004}. As shown in section [INSERT CC REF] the initial value problem \eqref{eq:num_ivp_1}, \eqref{eq:num_ivp_2} is equivelent to the Volterra equation.

\begin{align}
    \label{eq:num_int_eq}
    y(x) = \sum_{k=0}^{\lceil \alpha \rceil - 1} y_{0}^{(k)} \frac{x^k}{k!} + \frac{1}{\Gamma(\alpha)} \int_0^x (x-t)^{\alpha - 1} f(t, y(t))dt
\end{align}
In order to approximate the solution to this integral equation over the interval $ [0, T] $, we select a number of grid points $ N $ so that $ h = T / N $ and $ x_k = hk $ where $ k \in \{0, 1, ..., N\} $.

We apply the approximation
\begin{align*}
    \int_0^{x_{k+1}} (x_{k+1} - t)^{\alpha - 1} g(t)dt \approx \int_0^{x_{k+1}} (x_{k+1} - t)^{\alpha - 1} \tilde{g}_{k+1}(t)dt
\end{align*}
where $ g(t) = f(t, y(t)) $ and $ \tilde{g}_{k+1}(t) $ is the piecewise linear approximation of $ g(t) $ with nodes at the gridpoints $ x_k $. As outlined in \cite{Diethelm2004} we can approximate the integral in \eqref{eq:num_int_eq} as
\begin{align}
    \label{eq:amb_sum_1}
    \int_{0}^{x_{k+1}} (x_{k+1} - t)^{\alpha - 1} \tilde{g}_{k+1}(t) dt = \sum_{j=0}^{k+1} a_{j,k+1}g(x_j)
\end{align}
where 
\begin{align}
    a_{j,k+1} = \frac{h^\alpha}{\alpha(\alpha+1)} \times
    \begin{cases}
        (k^{\alpha+1}-(k-\alpha)(k+1)^\alpha) & \text{ if } j = 0 \\
        ((k-j+2)^{\alpha + 1} + (k-j)^{\alpha+1} - 2(k-j+1)^{\alpha+1}) & \text{ if } 1 \leq j \leq k \\
        1 & \text{ if } j = k + 1.
    \end{cases}
\end{align}

By seperating the final term in the sum we can write
\begin{align}
\label{eq:amb_y_corr}
    y_{k+1} = \sum_{j=0}^{\lceil \alpha \rceil - 1} y_{0}^{(j)} \frac{x^j_{k+1}}{j!} + \frac{1}{\Gamma(\alpha)} \left( \sum_{j=0}^k a_{j,k+1} f(x_j,y_j) + a_{k+1,k+1}f(x_{k+1}, y_{k+1}^P )\right)
\end{align}
where $ y_{k+1}^P $ is a \emph{predicted} value for $ y_{k+1} $ which must be calculated due to the potential non-linearity of $ f $ \cite{Diethelm2004}.

This predicted value is calculated by taking a rectangle approximation to the integral in \eqref{eq:num_int_eq}, to get
\begin{align}
    \label{eq:amb_sum_2}
    \int_{0}^{x_{k+1}} (x_{k+1} - t)^{\alpha - 1} g(t) dt \approx \sum_{j=0}^k b_{j,k+1}g(x_j)
\end{align}
where
\begin{align}
    b_{j,k+1} = \frac{h^\alpha}{\alpha} \left( (k+1-j)^\alpha - (k-j)^\alpha \right).
\end{align}

and thus a predicted value of $ y_{k+1} $ can be calculated by
\begin{align}
    \label{eq:amb_y_pred}
    y_{k+1}^P = \sum_{j=0}^{\lceil \alpha \rceil - 1} \frac{x^{j}_{k+1}}{j!} y_{0}^{(j)} + \frac{1}{\Gamma(\alpha)} \sum_{j=0}^{k} b_{j,k+1} f(x_j, y_j).
\end{align}

This outlines a fractional Adams Moulton Bashforth scheme. Of particular note are the sums which arise in \eqref{eq:amb_sum_1} and \eqref{eq:amb_sum_2}. These sums do not arise in the integer order Adams Moulton Bashforth method. They arise as a result of the non-local nature of the fractional derivative \cite{Diethelm2004}. These sums represent a significant computational cost as the number of terms grow linearly as the solution progress. This means in the fractional case the Adams Moulton Bashforth method has computational complexity $ O(N^2) $ \cite{Diethelm2011}. Section \ref{sec:parallel_c} will outline a task based parallel approach and section \ref{sec:parallel_cuda} will outline a massively parallel approach to solving \eqref{eq:num_ivp_1}, \eqref{eq:num_ivp_2} with NVIDIA's CUDA.
\subsubsection{Parallelizing: A Task Based Approach}
\label{sec:parallel_c}

Diethelm's paper \cite{Diethelm2011} outlines a parallel method of numerically approximating the solution to \eqref{eq:num_ivp_1}, \eqref{eq:num_ivp_2} by using a thread based parallel implementation of the Adams Moulton Bashforth method outlined in \ref{sec:amb_desc}. We base our approach in this section broadly on those of \cite{Diethelm2011} but reformulate the scheme in terms of tasks instead of threads. This has a number of distinct benefits including scalability, especially from an implementation standpoint and clarity.

After we have setup the gridpoints $ x_0, \ldots, x_{N-1} $ we create an array of solution values $ y_0, \ldots, y_{N-1} $ which are to be populated with the calculated solution. From the initial conditions we can immediatly calculate $ y_0 $. We then break up the vector (array) $ \mathbf{y} $ into blocks of size $ p $. Suppose that $ p = 2 $ for example. Then the first block would contain $ y_1 $ and $ y_2 $. Each of the $ p $ variables in each block can be calculated almost entirely in parallel. There is some dependency between values in each block (i.e. $ y_2 $ depends on $ y_1 $) but this can be done after the bulk of the parallel computations have been completed.

To illustrate this idea we consider figure \ref{fig:comp_diag}. We have broken the vector (array) $ \mathbf{y} $ up into $ K = \lceil \frac{N}{p} \rceil $ blocks of $ p $ variables.


\begin{figure}[h]
\input{diagrams/AMB_y_Diagram.tex}
\caption{Computation diagram for the values in the vector (array) $ \mathbf{y} $.}
\label{fig:comp_diag}
\end{figure} 

The centre row shows how $ \mathbf{y} $ is broken up into blocks, and the bottom row is a detailed view
of the contents of the \emph{current block} being computed.

The red lines indicate dependency, in that $ \mathbf{y}_{(\ell - 1)p + 1} $ depends on all the $ y_j $ values calculated in the previous blocks. $ \mathbf{y}_{(\ell - 1)p + 2} $ depends on all the $ y_j $ values calculated in previous blocks \emph{and} on $ \mathbf{y}_{(\ell - 1)p + 1 } $. 

The idea is that in each block we can do all of the computations which only depend on previous blocks in parallel and then perform the calculations which are dependent on other values within the same block in sequence. After all the values in a particular block are calculated we move on to the next block and repeat the process.

An exploded view of the last block is also provided to emphasise the fact that this block might not contain $ p $ variables if $ p $ does not divide $ N - 1 $. This fact causes us no bother as it is easy to handle in code.

As in \cite{Diethelm2011} we rewrite the equations \eqref{eq:amb_y_corr} and $ \eqref{eq:amb_y_pred} $ as
\begin{align}
y_{j+1}^P = I_{j+1} + h^\alpha H_{j,\ell}^P + h^\alpha L_{j,\ell}^P
\end{align}
and
\begin{align}
y_{j+1} = I_{j+1} + h^{\alpha} H_{j,\ell} + h^\alpha L_{j,\ell}
\end{align}

where
\begin{align}
I_{j+1} & := \sum_{k=0}^{\lceil \alpha \rceil -1} \frac{x_{j+1}^k}{k!} y_0^{(k)} \\
H^P_{j,\ell} & := \sum_{k=0}^{(\ell-1)p} b_{j-k} f(x_k, y_k) \\
L_{j,\ell}^P & := \sum_{k=(\ell-1)p+1}^{j} b_{j-k} f(x_k,y_k) \\
H_{j,\ell} & := c_j f(x_0, y_0) + \sum_{k=1}^{(\ell - 1)p} a_{j-k}f(x_k, y_k) \\
L_{j,\ell} & := \sum_{k=(\ell-1)p + 1}^j a{j-k} f(x_k, y_k) + \frac{f(x_{j+1}, y^P_{j+1})}{\Gamma(\alpha + 2.)}
\end{align}

In each block the values of $ H, I $ and $ H^P $ can be calculated in parallel for each $ y_j $ in the block.Then there is a somewhat more complex dependency between these sums. 

To best explain how the processes procedes in each block and what dependencies there are we will consider a specific example where there are two values to be calculated per block (i.e. $ p = 2 $). Consider figure \ref{fig:AMB_Task_Flow}. 

Each box in the figure represents a task. A task can execute when all the tasks with arrows pointing to it have completed. The little red number represents the index of the variable \emph{within} the block which is being calculated. All the task names are reasonablly self explanatory except for perhaps $ S^P $ and $ S $. These take the values calculated in the other sums and add them together to get $ y^P $ and $ y $ respectivly. These are very small tasks which take very little time to execute.

The red arrow illustrate the interdependency of variables within a block. The second variable cannot have the sum $ L^P $ calculate until the first variable is calculated. This means that in effect each of the $ L $ tasks have to execute in series but these sums are relatively small so the time taken is quite short. Each block is then executed in sequence as each block depends on the blocks before it. See Appendix A for a C\# implementation of this. 

\begin{figure}[H]
\input{diagrams/AMB_Task_Diagram.tex}
\caption{Task flow diagram for each block in the case when $ p = 2 $}
\label{fig:AMB_Task_Flow}
\end{figure}

\subsubsection{Analysis of the Performance of the Scheme in C\# and MONO}

We wish to analyse the performance of this scheme both from the a theoretical standpoint and from at practical standpoint. 

For the theoretical analysis of the scheme we introduce Amdahl's Law or Amdahl's principle.
\begin{principle}[Amdahl]
Let $ P $ be the proportion of a computation which can be parallelized in a given progress. Then the speedup given by
$ Q $ cores is
\begin{align*}
	S(Q) = \frac{1}{(1-P)+\frac{P}{Q}}
\end{align*}

\end{principle}

This law was first introduced in a 1967 paper by computer architect Amdahl. We introduce a new notion of Amdahl efficiency to characterise the how \emph{good} a parallel algorithm is in comparison with it serial counterpart in the limit of large input. 

To formalise this notion we introduce the following definition.

\begin{definition}[Amdahl Efficiency]
    Let $ S(N,Q) $ represent the parallel speedup for a problem with input size $ N $ and executed with $ Q $ cores.
    Then we define the Amdahl efficiency of parallel scheme as 
    \begin{align*}
        E(Q) = \lim_{N \lra \infty} \frac{S(N, Q)}{Q}.
    \end{align*}
    Further we say that a parallel scheme is Amdahl efficient with respect to its serial counterpart if $ E(p) = 1 $.
\end{definition}

\begin{proposition}
The parallel Adams Moulton Bashforth scheme is Amdahl efficient with respect to its serial counterpart.
\end{proposition}
\begin{proof}
We present the paralellizable fraction of computation in each block of variables in the following table. The order column represents the number of \emph{multiply-add} operations per task.

\begin{tabular}[ht]{|c|c|c|c|}
\hline
Task        & Computational Complexity  & Number of Variables per Block     & Total Complexity \\ \hline
$ H $       & $ O(\ell p ) $            & $ p $                             & $ O(\ell p^2) $ \\ \hline
$ H_p $     & $ O(\ell p ) $            & $ p $                             & $ O(\ell p^2) $ \\ \hline
$ I $       & $ O( \alpha ) $           & $ p $                             & $ O(p) $ \\ \hline \hline          
$ L $       & $ O( p ) $                & $ p $                             & $ O(p^2) $ \\ \hline
$ L_p $     & $ O( p ) $                & $ p $                             & $ O(p^2) $ \\ \hline
\end{tabular}

In each block $ H $, $ H_p $ and $ I $ can be computed in parallel across all variables in the block. For the purposes of simplicity we assume that the tasks $ L $ and $ L_p $ must all be computed in serial across all variables in a block. This means that in the $ \ell$-th block the non-parallelizable fraction of computation is
\begin{align}
    \overline{P}_{block}(\ell) \sim \frac{2p^2}{2p^2(\ell + 1) } \sim \frac{1}{\ell}
\end{align}

The amount of computation in the $ \ell$-th block is $ O(\ell p^2) $ and there are $ K \approx \frac{N}{p} $ blocks in the total computation. This means that the total amount of computation is
\begin{align}
    \sum_{\ell=1}^{K} \ell p^2 \sim \frac{K^2p^2}{2}
\end{align}
and by taking a weighted average sum, the total fraction of non-parallelizable computation is
\begin{align}
    \label{eq:tot_par_c}
    \overline{P}_{total} \sim \sum_{\ell = 1}^{K} \frac{2\ell p^2}{K^2 \ell p^2} = \frac{2}{K} = \frac{2p}{N}.
\end{align}

Noticing the fact that the computation in the task $ I $ is insignificant compared to $ H $ and $ H_p $ we would suppose using $ Q = 2p $ cores would be in some sense optimal. Applying Amdahl's principle with the result in \ref{eq:tot_par_c} and with $ Q = 2p $ we get 
\begin{align*}
    S(N, Q) &= S(N, 2p) \\
    &\sim \frac{1}{(\frac{2p}{N}) + \frac{1-\frac{2p}{N}}{2p} } \\
\end{align*}
then letting $ N \lra \infty $ we get
\begin{align*}
    E(p) = \lim_{N \lra \infty} \left(\frac{1}{(\frac{2p}{N}) + \frac{1-\frac{2p}{N}}{2p} } \right) \slash p = 1.
\end{align*}

\end{proof}

What this result means is that if we have $ 2p $ free cores this parallel method should take $ \frac{1}{2p} $ the amount of time to run compared to it's serial counterpart given a sufficiently large number of grid points, $N$. Due to the fact that this a task based scheme this is also dependent on an efficient scheduler.

We now compare this with practical experimentation. This method was developed in

\begin{figure}[H]
\includegraphics[scale = 0.3]{images/Dot_Net_Performance_No_Label}
\caption{Average runtime of the scheme for different values of $ p $ and $ N $}
\label{fig:mono_performance_surface}
\end{figure}

Firstly the $ O(N^2) $ complexity of the algorithm is quite clearly visible, especially in the case where $ p = 1 $. Secondly the law of diminishing returns is also clearly visible in this plot. This affect can be attributed to Amdahl's law c.f. \cite{Rogers1985}. 

In the case where $ p = 1 $ it should be noted that there are up to $ 4 $ tasks running just for that one variable ($ I $, $ L^P $, $ H $, $ H^P $). In Diethelm's method, $ p = 1 $ would refer to only 1 thread running at all times, which is not the case here. While this limits the validity of any comparisons made with the method of Diethelm, it increases scalability and flexability.


\subsubsection{Parallelizing: A Naive CUDA Approach}
\label{sec:parallel_cuda}

\subsubsection{Reducing the Complexity Class: Speed - Accuracy Tradeoffs}
\label{sec:reduce_complexity}

\clearpage
